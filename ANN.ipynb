{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregularizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m l2\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import joblib\n",
    "\n",
    "# Suppress specific warnings from Keras and other libraries\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='keras')\n",
    "\n",
    "# Load the data from a CSV file\n",
    "data = pd.read_csv('PST_v6.csv')  # Replace 'topography.csv' with the actual file name\n",
    "\n",
    "# Remove commas and convert to numeric values\n",
    "data = data.replace(',', '', regex=True).apply(pd.to_numeric, errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Define input (X) and output (y) variables\n",
    "X = data[['cpower1', 'cspeed1', 'cpower2','cspeed2','angle']]\n",
    "y = data[['eta_SID', 'rho_s_SID', 'sig_s_SID', 'Sa', 'Sq', 'Ssk', 'Sku', 'sigma_s_rho','Sdq','Sdr']]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (50, 100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Function to calculate normalized RMSE and MAE\n",
    "def normalized_rmse_mae(y_true, y_pred):\n",
    "    range_actual = y_true.max() - y_true.min()  # Range of actual values for normalization\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Normalize RMSE and MAE\n",
    "    normalized_rmse = rmse / range_actual\n",
    "    normalized_mae = mae / range_actual\n",
    "    \n",
    "    return normalized_rmse, normalized_mae\n",
    "\n",
    "# Custom wrapper class to fully implement scikit-learn's API\n",
    "class KerasModelWrapper:\n",
    "    def __init__(self, hidden_layer_sizes=(50, 100, 50), activation='relu', learning_rate=0.001, epochs=200, batch_size=32):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def build_model(self, input_dim):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))\n",
    "        for units in self.hidden_layer_sizes:\n",
    "            model.add(tf.keras.layers.Dense(units, activation=self.activation, kernel_regularizer=l2(0.001)))  # L2 Regularization\n",
    "        model.add(tf.keras.layers.Dense(1))  # Output layer for regression\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = self.build_model(input_dim)\n",
    "        \n",
    "        # Fit the model and store the history\n",
    "        self.history = self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0, validation_split=0.1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)\n",
    "\n",
    "    def save(self, filename):\n",
    "        # Save the model as a .pkl file using joblib\n",
    "        joblib.dump(self, filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        # Load the model from a .pkl file\n",
    "        return joblib.load(filename)\n",
    "\n",
    "# 10-fold Cross-validation setup\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop through each target variable\n",
    "for i, col in enumerate(y.columns):\n",
    "    \n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for hidden_layer_sizes in param_grid['hidden_layer_sizes']:\n",
    "        for activation in param_grid['activation']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                \n",
    "                # Perform cross-validation manually\n",
    "                cv_r2_scores = []\n",
    "                \n",
    "                for train_index, test_index in kf.split(X_train_scaled):\n",
    "                    X_train_fold, X_test_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "                    y_train_fold, y_test_fold = y_train.iloc[train_index, i], y_train.iloc[test_index, i]\n",
    "\n",
    "                    # Train and evaluate the model\n",
    "                    model = KerasModelWrapper(hidden_layer_sizes=hidden_layer_sizes, activation=activation, learning_rate=learning_rate)\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "                    \n",
    "                    # Evaluate the model on the test fold\n",
    "                    y_pred_fold = model.predict(X_test_fold)\n",
    "                    r2_fold = r2_score(y_test_fold, y_pred_fold)\n",
    "                    \n",
    "                    cv_r2_scores.append(r2_fold)\n",
    "                \n",
    "                # Average cross-validated R²\n",
    "                mean_r2 = np.mean(cv_r2_scores)\n",
    "                if mean_r2 > best_r2:\n",
    "                    best_r2 = mean_r2\n",
    "                    best_model = model\n",
    "                    best_params = {\n",
    "                        'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                        'activation': activation,\n",
    "                        'learning_rate': learning_rate\n",
    "                    }\n",
    "\n",
    "    # Final evaluation on the test set\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    r2 = best_model.score(X_test_scaled, y_test.iloc[:, i])\n",
    "    normalized_rmse, normalized_mae = normalized_rmse_mae(y_test.iloc[:, i], y_pred)\n",
    "\n",
    "    # Suppress intermediate output and only show relevant metrics and plots\n",
    "    print(f\"{col} - R² Score: {r2:.4f}, Normalized RMSE: {normalized_rmse:.4f}, Normalized MAE: {normalized_mae:.4f}\")\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(best_model.history.history['loss'], label='Training Loss')\n",
    "    plt.plot(best_model.history.history['val_loss'], label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epoch', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.title(f'Training and Validation Loss for {col}', fontsize=20)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test.iloc[:, i], y_pred, color='blue')\n",
    "    plt.plot([y_test.iloc[:, i].min(), y_test.iloc[:, i].max()], \n",
    "             [y_test.iloc[:, i].min(), y_test.iloc[:, i].max()], color='red', lw=2)\n",
    "    plt.xlabel(f'Actual {col}', fontsize=20)\n",
    "    plt.ylabel(f'Predicted {col}', fontsize=20)\n",
    "    plt.title(f'Actual vs Predicted for {col}', fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate Permutation Feature Importance (PFI) using the original input features\n",
    "    pfi = permutation_importance(best_model, X_test_scaled, y_test.iloc[:, i], n_repeats=10, random_state=42)\n",
    "\n",
    "    # Plot the PFI results using the original input features\n",
    "    feature_importances = pd.Series(pfi.importances_mean, index=X.columns)\n",
    "    feature_importances = feature_importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importances.index, feature_importances.values)\n",
    "    plt.title(f'Permutation Feature Importance for {col}', fontsize=20)\n",
    "    plt.xlabel('Mean Importance Score', fontsize=18)\n",
    "    plt.ylabel('Feature', fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python.defaultInterpreterPath': 'C:\\\\Users\\\\<YourUsername>\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"python.defaultInterpreterPath\": \"C:\\\\Users\\\\<YourUsername>\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python.exe\"\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
